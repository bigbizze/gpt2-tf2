{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ftfy import fix_text\n",
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import glob\n",
    "import csv        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(\"../data/scraped/*.txt\") \n",
    "PROCESS_DATA_PATH = \"../data/processed.txt\"\n",
    "BPE_TSV_PATH = \"../data/bpe_spm.tsv'\"\n",
    "BPE_MODEL_PATH = \"../data/bpe_model\"\n",
    "VOCAB_SIZE = 32000\n",
    "TF_RECORDS = \"../data/tf_records/\"\n",
    "BOS_ID = 3\n",
    "EOS_ID = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:04<00:00, 150.24it/s]\n"
     ]
    }
   ],
   "source": [
    "file_writer = open(PROCESS_DATA_PATH, \"w\")\n",
    "for file_name in tqdm.tqdm(text_files):\n",
    "    fr = open(file_name,'r')\n",
    "    file_writer.writelines([fix_text(line, normalization='NFKC') for line in fr.readlines()])\n",
    "    fr.close\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30411it [00:00, 143923.24it/s]\n"
     ]
    }
   ],
   "source": [
    "token_dict = Counter()\n",
    "with open(PROCESS_DATA_PATH,'r') as fr:\n",
    "    for line in tqdm.tqdm(fr):\n",
    "        token_dict.update(line.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BPE_TSV_PATH, 'w', newline='') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "    for word in token_dict:\n",
    "        tsv_output.writerow([word, token_dict[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model = \"/disk4/snm2.0_2019/data/edge_corpus/edge_spm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmcmd = '--input={spm_input} --model_prefix={spm_model} --input_format=tsv --vocab_size={vocab_size} --user_defined_symbols=[SEP],[BOS],[EOS] --hard_vocab_limit=false --model_type=bpe --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --pad_piece=[PAD] --unk_piece=[UNK]'.format(spm_input=BPE_TSV_PATH, spm_model=BPE_MODEL_PATH, vocab_size=VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(spmcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(BPE_MODEL_PATH + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '[EOS]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.encode_as_pieces(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_LEN = 10\n",
    "MAX_SEQ_LEN = 512\n",
    "per_file_limit = 100000\n",
    "filename = TF_RECORDS + str(datetime.datetime.now().timestamp()) + \".tfrecord\"\n",
    "tf_writer = tf.io.TFRecordWriter(filename)\n",
    "doc_counts = 0\n",
    "\n",
    "with open(PROCESS_DATA_PATH,'r') as f:\n",
    "    for line in f:\n",
    "        encoded_id = s.encode_as_ids(line)\n",
    "        if len(encoded_id) < MAX_SEQ_LEN and len(encoded_id) > 10:\n",
    "            inputs = np.array([BOS_ID] + encoded_id)\n",
    "            targets = np.array(encoded_id + [EOS_ID])\n",
    "            \n",
    "            example = serialize_example(inputs, targets)\n",
    "            tf_writer.write(example)\n",
    "            doc_counts +=1\n",
    "        if doc_counts >= per_file_limit:\n",
    "            tf_writer.write(example)\n",
    "            doc_counts = 0\n",
    "            tf_writer.close()\n",
    "            filename = output_dir + str(datetime.datetime.now().timestamp()) + \".tfrecord\"\n",
    "            tf_writer = tf.io.TFRecordWriter(filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def serialize_example(inputs, targets):\n",
    "    feature = {\n",
    "        'inputs': _int64_feature(inputs),\n",
    "        'targets': _int64_feature(targets)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x12\\n\\x10\\n\\x06inputs\\x12\\x06\\x1a\\x04\\n\\x02\\x02\\x03'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialize_example([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(serialized_example):\n",
    "    data_fields = {\n",
    "        \"inputs\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"targets\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(serialized_example, data_fields)\n",
    "    inputs = tf.sparse.to_dense(parsed[\"inputs\"])\n",
    "    targets = tf.sparse.to_dense(parsed[\"targets\"])\n",
    "\n",
    "    inputs = tf.cast(inputs, tf.int32)\n",
    "    targets = tf.cast(targets, tf.int32)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "def input_fn(tf_recods, batch_size=32, padded_shapes= ([-1], [-1]), epoch=10, buffer_size=10000):\n",
    "    if type(tf_recods) is str:\n",
    "        tf_recods = [tf_recods]\n",
    "    dataset = tf.data.TFRecordDataset(tf_recods, buffer_size=10000)\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = dataset.map(parse_example)\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n",
    "    dataset = dataset.repeat(epoch)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/tf_records/1566726380.168402.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "dataset = input_fn(glob.glob((TF_RECORDS + \"*.tfrecord\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    3  3419 31904 ...     0     0     0]\n",
      " [    3   100   483 ...     0     0     0]\n",
      " [    3    64 31904 ...     0     0     0]\n",
      " ...\n",
      " [    3   108  1438 ...     0     0     0]\n",
      " [    3   100 14810 ...     0     0     0]\n",
      " [    3   101 31904 ...     0     0     0]], shape=(32, 339), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3419 31904 31883 ...     0     0     0]\n",
      " [  100   483  4482 ...     0     0     0]\n",
      " [   64 31904 31890 ...     0     0     0]\n",
      " ...\n",
      " [  108  1438  1694 ...     0     0     0]\n",
      " [  100 14810  6112 ...     0     0     0]\n",
      " [  101 31904  3620 ...     0     0     0]], shape=(32, 339), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inp, tar in dataset:\n",
    "    print(inp)\n",
    "    print(tar)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
